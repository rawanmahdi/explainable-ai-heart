{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "heart_csv_path = 'C:/Users/Rawan Alamily/Downloads/McSCert Co-op/explainable-ai-heart/models/lifestyle-metrics-model/data/life-heart.csv'\n",
    "dataframe = pd.read_csv(heart_csv_path)\n",
    "print(dataframe.describe())\n",
    "print(dataframe.shape)\n",
    "dataframe['target'] = np.where(dataframe['heartDisease']=='Yes', 1, 0)\n",
    "dataframe = dataframe.drop(columns=['heartDisease'])\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "neg, pos = np.bincount(dataframe['target'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 bmi  physicalHealth   mentalHealth     sleepHours\n",
      "count  319795.000000    319795.00000  319795.000000  319795.000000\n",
      "mean       28.325399         3.37171       3.898366       7.097075\n",
      "std         6.356100         7.95085       7.955235       1.436007\n",
      "min        12.020000         0.00000       0.000000       1.000000\n",
      "25%        24.030000         0.00000       0.000000       6.000000\n",
      "50%        27.340000         0.00000       0.000000       7.000000\n",
      "75%        31.420000         2.00000       3.000000       8.000000\n",
      "max        94.850000        30.00000      30.000000      24.000000\n",
      "(319795, 17)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def df_to_dataset(df, batch_size=32, resample=False):\n",
    "    df = df.copy()\n",
    "    if resample:\n",
    "        pos_df = df[df['target'] == 1]\n",
    "        neg_df = df[df['target'] == 0]\n",
    "        pos_labels = pos_df.pop('target')\n",
    "        pos_features = pos_df\n",
    "        neg_labels = neg_df.pop('target')\n",
    "        neg_features = neg_df\n",
    "        pos_ds = tf.data.Dataset.from_tensor_slices((dict(pos_features), pos_labels))\n",
    "        neg_ds = tf.data.Dataset.from_tensor_slices((dict(neg_features), neg_labels))\n",
    "        \n",
    "        resampled_ds = tf.data.Dataset.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\n",
    "        #resampled_ds = resampled_ds.apply(tf.data.experimental.assert_cardinality(54748))\n",
    "        resampled_ds = resampled_ds.shuffle(buffer_size=len(df))\n",
    "        resampled_ds = resampled_ds.batch(batch_size).prefetch(2).repeat()\n",
    "        return resampled_ds\n",
    "    else:\n",
    "        labels = df.pop('target')\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices((dict(df), labels)).cache()\n",
    "        shuffled_tf_dataset = tf_dataset.shuffle(buffer_size=len(df)) # shuffling values \n",
    "        return tf_dataset.batch(batch_size).prefetch(2)# returning 32 samples per batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# with large batch size\n",
    "batch_size=64\n",
    "train, val, test = np.split(dataframe.sample(frac=1), [int(0.8*len(dataframe)), int(0.9*len(dataframe))])\n",
    "dataframe = dataframe.sample(frac=1)\n",
    "#train, val = train_test_split(dataframe, test_size=0.3, random_state=RANDOM_SEED)\n",
    "resampled_train_ds = df_to_dataset(df=train, batch_size=batch_size, resample=True)\n",
    "val_ds = df_to_dataset(df=val, batch_size=batch_size, resample=False)\n",
    "test_ds = df_to_dataset(df=test, batch_size=batch_size)\n",
    "steps_per_epoch = np.ceil(2.0*pos/batch_size)\n",
    "print(steps_per_epoch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "856.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_normalization_layer(feature_name, dataset):\n",
    "    # normalize numeric features\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "    # extract feature from dataset\n",
    "    feature_ds = dataset.map(lambda x, y: x[feature_name])\n",
    "    normalizer.adapt(feature_ds, batch_size=batch_size, steps=steps_per_epoch)\n",
    "    return normalizer\n",
    "def get_category_encoding_layer(feature_name, dataset, dtype, max_tokens=None):\n",
    "    if dtype == 'string':\n",
    "        index = layers.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "    # extract feature from dataset\n",
    "    feature_ds = dataset.map(lambda x, y: x[feature_name])\n",
    "    # 'learn' all possible feature values, assign each an int index \n",
    "    index.adapt(feature_ds, batch_size=batch_size, steps=steps_per_epoch)\n",
    "    # encode integer index\n",
    "    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size(), output_mode=\"one_hot\")\n",
    "    # multi-hot encode indeices - lambda function captures layers\n",
    "    return lambda feature: encoder(index(feature))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# preprocess all features:\n",
    "inputs = []\n",
    "encoded_features =[]\n",
    "\n",
    "# numerical\n",
    "for header in [\"bmi\", \"physicalHealth\", \"mentalHealth\", 'sleepHours' ]:\n",
    "    num_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    # keras inputs array\n",
    "    inputs.append(num_col)\n",
    "\n",
    "    norm_layer = get_normalization_layer(feature_name=header, dataset=resampled_train_ds)\n",
    "    encoded_num_col = norm_layer(num_col)\n",
    "    # encoded feature\n",
    "    encoded_features.append(encoded_num_col)\n",
    "\n",
    "# categorical\n",
    "for header in [\"smoking\",\"alcoholDrinking\",\"stroke\",\"diffWalk\",\n",
    "                \"sex\", \"ageGroup\", \"diabetic\", \"physicalActivity\", \n",
    "                \"overallHealth\", \"asthma\", \"kidneyDisease\", \"skinCancer\"]:\n",
    "    \n",
    "    # declare header as a keras Input\n",
    "    cat_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "    # keras inputs array\n",
    "    inputs.append(cat_col)\n",
    "\n",
    "    # get preprocessing layer \n",
    "    cat_layer = get_category_encoding_layer(feature_name=header,\n",
    "                                            dataset=resampled_train_ds, \n",
    "                                            dtype='string', \n",
    "                                            max_tokens=None)\n",
    "    encoded_cat_col = cat_layer(cat_col)\n",
    "    # encoded feature\n",
    "    encoded_features.append(encoded_cat_col)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# KERAS FUNCTIONAL API - MODEL BUILD   \n",
    "# merge list feature inputs into one vector\n",
    "features = tf.keras.layers.concatenate(encoded_features)\n",
    "x = tf.keras.layers.Dense(units=128, activation=\"relu\")(features)\n",
    "x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "x = tf.keras.layers.Dense(units=128, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(units=1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(inputs, output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "result = model.fit(resampled_train_ds, \n",
    "                    validation_data=val_ds, \n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    use_multiprocessing=True, \n",
    "                    verbose=1)\n",
    "\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "binary_predictions = tf.round(predictions).numpy().flatten()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "856/856 [==============================] - 16s 9ms/step - loss: 0.2434 - accuracy: 0.9132 - val_loss: 0.2248 - val_accuracy: 0.9172\n",
      "Epoch 2/20\n",
      "856/856 [==============================] - 7s 8ms/step - loss: 0.2245 - accuracy: 0.9188 - val_loss: 0.2243 - val_accuracy: 0.9177\n",
      "Epoch 3/20\n",
      "856/856 [==============================] - 7s 8ms/step - loss: 0.2296 - accuracy: 0.9148 - val_loss: 0.2229 - val_accuracy: 0.9172\n",
      "Epoch 4/20\n",
      "856/856 [==============================] - 8s 9ms/step - loss: 0.2317 - accuracy: 0.9146 - val_loss: 0.2236 - val_accuracy: 0.9174\n",
      "Epoch 5/20\n",
      "856/856 [==============================] - 13s 15ms/step - loss: 0.2269 - accuracy: 0.9165 - val_loss: 0.2238 - val_accuracy: 0.9173\n",
      "Epoch 6/20\n",
      "856/856 [==============================] - 8s 10ms/step - loss: 0.2294 - accuracy: 0.9151 - val_loss: 0.2241 - val_accuracy: 0.9174\n",
      "Epoch 7/20\n",
      "856/856 [==============================] - 8s 10ms/step - loss: 0.2311 - accuracy: 0.9140 - val_loss: 0.2231 - val_accuracy: 0.9175\n",
      "Epoch 8/20\n",
      "856/856 [==============================] - 8s 10ms/step - loss: 0.2282 - accuracy: 0.9165 - val_loss: 0.2223 - val_accuracy: 0.9171\n",
      "Epoch 9/20\n",
      "856/856 [==============================] - 8s 9ms/step - loss: 0.2244 - accuracy: 0.9171 - val_loss: 0.2227 - val_accuracy: 0.9172\n",
      "Epoch 10/20\n",
      "856/856 [==============================] - 14s 16ms/step - loss: 0.2259 - accuracy: 0.9165 - val_loss: 0.2227 - val_accuracy: 0.9169\n",
      "Epoch 11/20\n",
      "856/856 [==============================] - 9s 10ms/step - loss: 0.2255 - accuracy: 0.9171 - val_loss: 0.2231 - val_accuracy: 0.9172\n",
      "Epoch 12/20\n",
      "856/856 [==============================] - 9s 10ms/step - loss: 0.2276 - accuracy: 0.9151 - val_loss: 0.2221 - val_accuracy: 0.9171\n",
      "Epoch 13/20\n",
      "856/856 [==============================] - 9s 11ms/step - loss: 0.2284 - accuracy: 0.9156 - val_loss: 0.2224 - val_accuracy: 0.9176\n",
      "Epoch 14/20\n",
      "856/856 [==============================] - 9s 10ms/step - loss: 0.2249 - accuracy: 0.9164 - val_loss: 0.2223 - val_accuracy: 0.9170\n",
      "Epoch 15/20\n",
      "856/856 [==============================] - 12s 14ms/step - loss: 0.2284 - accuracy: 0.9151 - val_loss: 0.2225 - val_accuracy: 0.9174\n",
      "Epoch 16/20\n",
      "856/856 [==============================] - 9s 11ms/step - loss: 0.2238 - accuracy: 0.9167 - val_loss: 0.2226 - val_accuracy: 0.9170\n",
      "Epoch 17/20\n",
      "856/856 [==============================] - 9s 11ms/step - loss: 0.2276 - accuracy: 0.9169 - val_loss: 0.2222 - val_accuracy: 0.9166\n",
      "Epoch 18/20\n",
      "856/856 [==============================] - 9s 11ms/step - loss: 0.2276 - accuracy: 0.9148 - val_loss: 0.2228 - val_accuracy: 0.9173\n",
      "Epoch 19/20\n",
      "856/856 [==============================] - 15s 17ms/step - loss: 0.2287 - accuracy: 0.9159 - val_loss: 0.2224 - val_accuracy: 0.9170\n",
      "Epoch 20/20\n",
      "856/856 [==============================] - 9s 10ms/step - loss: 0.2248 - accuracy: 0.9169 - val_loss: 0.2228 - val_accuracy: 0.9179\n",
      "500/500 [==============================] - 3s 5ms/step\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(classification_report(test.get('target'), binary_predictions))\n",
    "# layer connectivity visualization\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96     29200\n",
      "           1       0.60      0.09      0.15      2780\n",
      "\n",
      "    accuracy                           0.92     31980\n",
      "   macro avg       0.76      0.54      0.55     31980\n",
      "weighted avg       0.89      0.92      0.89     31980\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}